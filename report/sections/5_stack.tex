%!TEX root=../main.tex

\section{Linearizability for the Stack Data Structure}\label{stack-section}

We define the signature of a ``Last In, First Out'' (LIFO) stack object to be the following:
\begin{enumerate*}
    \item $\push(x) \Rightarrow \allowbreak \true$ inserts $x$ onto the top of the stack
    \item $\peek() \Rightarrow x$ retrieves the value at the top of the stack that is currently $x$
    \item $\pop() \Rightarrow x$ removes the value from the top of the stack that is currently $x$.
\end{enumerate*}
The stack is initialized to be empty. We define the ML problem on a stack to be the following:

\begin{problem*}[Monitoring Linearizability for Stack (MLS)]
   Given a stack history $H$ with distinct values, is $H$ linearizable?
\end{problem*}

To understand the linearizability of a stack history, it is essential to establish some properties of a legal linearization. In particular, we observe that the relative positions of any pair of elements within a stack stays consistent throughout their lifetimes in the data type. In essence, if element $a$ is on top of $b$ in the stack at some point in time, there exists no point in time where $b$ can be on top of $a$. As a simple illustration of why it is the case, suppose $a$ is on top of $b$ at some point in time. It is clear that $b$ must be inserted first. However, the assumption that $b$ is on top of $a$ at another point in time asserts that we have $a$ inserted before $b$, which is a contradiction as each value is inserted into the stack at most once according to the distinct value restriction. We can use this definitive behaviour of the consistent relative positions of elements within the data type to derive the algorithm.

Given a stack history $H$, let $l$ be some linearization such that $l$ is legal on $H_v$ for $v \in \vals_H$. We define $<_l$ as a binary relation on values such that $v_1 <_l v_2$ if there is an operation $o_2$, $\valOf{o_2} = v_2$, such that $l(o_{\push(v_1)}) < l(o_2) < l(o_{\pop(v_1)})$.

\begin{proposition}\label{stack-partial-def}
    $l$ is legal iff $<_l$ is a partial order.
\end{proposition}

Intuitively, $l(o_{\push(v_1)}) < l(o_2) < l(o_{\pop(v_1)})$ denotes $v_2$ being accessed at the top of the stack while $v_1$ is present. In other words, $v_1 <_l v_2$ implies $v_1$ to be conceptually below $v_2$ at some point in the linearization $l$. Since any given stack history is assumed to be well-matched, we are guaranteed that $<_l$ is complete in capturing all such relations. We can now see that Proposition \ref{reduce-one-value} follows naturally from Proposition \ref{stack-partial-def}. Consequently, there must also exists a ``lowest value'' in terms of the partial order $<_l$ given by any legal linearization $l$.

\begin{corollary}[Bottom Values]
    Let $H$ be a stack history with legal linearization $l$. Then there must exist a value $v$ such that $H_v = \set{o_{\push(v)}, o_{\pop(v)}, o_{\peek_1(v)}, o_{\peek_2(v)}, ..., o_{\peek_k(v)}}$, $l(o_{\push(v)}) < l(o_{\peek_1(v)}) < l(o_{\peek_2(v)}) < ... < l(o_{\peek_k(v)}) < l(o_{\pop(v)})$, and for all other values $v' \neq v$, one of the following holds:
    \begin{enumerate}
        \item for all $o' \in H_{v'}$, $l(o') < l(o_{\push(v)})$
        \item for all $o' \in H_{v'}$, $l(o_{\push(v)}) < l(o') < l(o_{\peek_1(v)})$
        \item for all $o' \in H_{v'}$, $l(o_{\peek_i(v)}) < l(o') < l(o_{\peek_{i+1}(v)})$ for some $1 \leq i < k$
        \item for all $o' \in H_{v'}$, $l(o_{\peek_k(v)}) < l(o') < l(o_{\pop(v)})$
        \item for all $o' \in H_{v'}$, $l(o_{\pop(v)}) < l(o')$
    \end{enumerate}
\end{corollary}

\begin{corollary}\label{lin-exist-v}
    Let $H$ be a stack history with legal linearization $l$. Then there must exist a value $v$ such that for all other values $v' \neq v$ and $o \in H_v$, one of the following holds:
    \begin{enumerate}
        \item for all $o' \in H_{v'}$, $\invTimeOf{o'} < l(o)$, or
        \item for all $o' \in H_{v'}$, $l(o) < \resTimeOf{o'}$
    \end{enumerate}
\end{corollary}

Such a value $v$ is also called the \emph{bottom value} of history $H$, signifying that the value is ``at the bottom'' of the stack in some legal linearization of $H$.

\begin{lemma}\label{exist-v-lin}
    Let $H$ be a stack history. Let $v$ be a value and $l$ be a legal linearization of $H_v$ such that for all other values $v' \neq v$ and $o \in H_v$, one of the following holds:
    \begin{enumerate}
        \item for all $o' \in H_{v'}$, $\invTimeOf{o'} < l(o)$, or
        \item for all $o' \in H_{v'}$, $l(o) < \resTimeOf{o'}$
    \end{enumerate}
    Then $H$ is linearizable iff $H \setminus H_v$ is linearizable.
\end{lemma}
\begin{proof}
    \begin{itemize}
        \item[($\Rightarrow$)] See Proposition \ref{reduce-one-value}.
        \item[($\Leftarrow$)] Let $H_v = \set{o_1, o_2, ..., o_m}$, $t_i = l(o_i)$ and $G_i$ denote the union of the set $\set{H_{v_{i1}}, H_{v_{i2}}, ..., H_{v_{ik_i}}}$ for which $\invTimeOf{o'} < t_i$ for all $o' \in H_{v_{ij}}$, $1 \leq i \leq m$, $1 \leq j \leq k_i$. Without loss of generality, assume $t_i < t_{i+1}$ for all $1 \leq i < m$. It is clear that for each $1 \leq i \leq m$, $G_i'' = G_i' \setminus \bigcup_{1\leq j < i}{G_j'}$ is also well-matched, and since $G_i'' \subseteq H \setminus H_v$, it is also linearizable by Corollary \ref{reduce-well-matched}. Notice that for all $o'\in G_i''$, $t_{i-1} < \resTimeOf{o'}$ and $\invTimeOf{o'} < t_i$. Hence, there exists a legal linearization $l_i$ of $G_i''$ such that $t_{i-1} < l_i(o') < t_i$ by Lemma \ref{tight-hist} (assume $t_0$ to be 0).

Similarly, there must also exist a legal linearization $l_{m+1}$ of $G_{m+1}'' = H \setminus \bigcup_{1\leq j \leq m}{G_j'}$ such that $t_m < l_{m+1}(o)$ for all $o \in G_{m+1}''$. Let $l_v$ be a linearization of $H_v$ where $l_v(o_i) = t_i$. Reader may verify that the union of the linearizations $l_v, l_1, l_2, ..., l_{m+1}$ is indeed a legal linearization of $H$ by Proposition \ref{stack-partial-def}.
    \end{itemize}
\end{proof}

Intuitively, a linearization of a linearizable stack history emits a non-empty set of values $S$ that resides at the bottom of the stack at some point in the linearization. By Corollary \ref{lin-exist-v}, each operation $o$ where $\valOf{o} \in S$ must be scheduled at some time $t$, when no other values are necessarily in the stack. By contrapositive, if no such set $S$ exists, $H$ is not linearizable. It also turns out that if such a set $S$ exists, then $H$ is linearizable iff $\set{o\in H\;|\;\valOf{o} \notin S}$ is linearizable by Lemma \ref{exist-v-lin}.

Using this observation, we derive a polynomial time algorithm for deciding a given stack history's linearizability as described below.

\input{algos/stack-lin}

\begin{theorem}\label{mls-dist}
    MLS is solvable in $O(n\log{n})$.
\end{theorem}
\begin{proof}
\algoref{stack-algo} depicts an non-optimized version of the algorithm. In this algorithm, we are given a simplified well-matched stack history, $H$, with distinct values. The initialization and population of $OpByVal$ and $Events$ takes $O(n \log{n})$ in total, accounting for the sorting of $Events$. The updates of $RunningOp$, $CritVal$ and $EndedVal$ in a single iteration of the for loop on $Events$ runs in amortized $O(1)$ time. It is guaranteed that the algorithm terminates within $O(n)$ iterations, giving the algorithm a total time complexity of $O(n^2 \log{n})$.

The correctness of the algorithm is given by Corollary \ref{lin-exist-v} and Lemma \ref{exist-v-lin}.
Intuitively, we greedily schedule operations satisfying Lemma \ref{exist-v-lin}. If all operations on a value $v$ is scheduled, we are guaranteed that a linearization of $H_v$ satisfying Lemma \ref{exist-v-lin} exists.

Reader may notice that the recursion unnecessarily reconstructs and sorts $OpByVal$ and $Events$. It follows that simply reusing a populated and sorted $Events$ throughout different iterations of $\StackLin$ already give you a lower total time complexity of $O(n^2)$. Furthermore, any operations removed from $OpByVal$ within an iteration of $\StackLin$ will also necessarily be removed in the next recursion. 

The intuition behind an optimized $O(n\log{n})$ time complexity lies on the use of segment trees to efficiently retrieve time intervals for which $CritVal$ is empty, or contains solely a single value, instead of having to iterate through $Events$. An interval tree is then used as a persistent version of $OpByVal$ to keep track of unremoved operations, supporting an efficient $O(m\log{n})$ time removal of operations on a given interval where $CritVal$ is empty, or contains a single value (where $m$ is the number of operations removed). Refer to Appendix \ref{stack-fast} for more details.
\end{proof}